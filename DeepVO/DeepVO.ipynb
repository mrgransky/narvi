{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get resized image from a provided path\n",
    "def get_image(path,img_size= (1280,384)):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, img_size, cv2.INTER_LINEAR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to laod images from a given directory and forming batches\n",
    "def load_images(img_dir, img_size):\n",
    "    print (\"images \", img_dir)\n",
    "    images= []\n",
    "    images_set =[]\n",
    "    for img in glob.glob(img_dir+'/*'):\n",
    "        images.append(get_image(img,img_size))\n",
    "    for i in range(len(images)-1):\n",
    "        img1 = images[i]\n",
    "        img2 = images[i+1]\n",
    "        img = np.concatenate([img1, img2],axis = -1)\n",
    "        images_set.append(img)\n",
    "    print(\"images count : \",len(images_set))\n",
    "    images_set = np.reshape(images_set, (-1, 6, 384, 1280))\n",
    "    return images_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for pose preprocessing \n",
    "def isRotationMatrix(R):\n",
    "    \"\"\" Checks if a matrix is a valid rotation matrix\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "    \"\"\"\n",
    "    Rt = np.transpose(R)\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    return n < 1e-6\n",
    "\n",
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\" calculates rotation matrix to euler angles\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles\n",
    "    \"\"\"\n",
    "    assert(isRotationMatrix(R))\n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "def getMatrices(all_poses):\n",
    "    all_matrices = []\n",
    "    for i in range(len(all_poses)):\n",
    "        #print(\"I: \",i)\n",
    "        j = all_poses[i]\n",
    "        #print(\"J:   \",j)\n",
    "        p = np.array([j[3], j[7], j[11]])\n",
    "        #print(\"P:   \", p)\n",
    "        R = np.array([[j[0],j[1],j[2]],\n",
    "                [j[4],j[5],j[6]],\n",
    "                [j[8],j[9],j[10]]])\n",
    "        #print(\"R:   \", R)\n",
    "        angles = rotationMatrixToEulerAngles(R)\n",
    "        #print(\"Angles: \",angles)\n",
    "        matrix = np.concatenate((p,angles))\n",
    "        #print(\"MATRIX: \", matrix)\n",
    "        all_matrices.append(matrix)\n",
    "    return all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to get poses form a given location \n",
    "def load_poses(pose_file):\n",
    "    print (\"pose \",pose_file)\n",
    "    poses = []\n",
    "    poses_set = []\n",
    "    with open(pose_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose = np.fromstring(line, dtype=float, sep=' ')\n",
    "            poses.append(pose)\n",
    "    poses = getMatrices(poses)\n",
    "    for i in range(len(poses)-1):\n",
    "        pose1 = poses[i]\n",
    "        pose2 = poses[i+1]\n",
    "        finalpose = pose2-pose1\n",
    "        poses_set.append(finalpose)\n",
    "    print(\"poses count: \",len(poses_set))\n",
    "    return poses_set          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary dataloader function to get both images and poses\n",
    "def VODataLoader(datapath,img_size= (1280,384), test=False):\n",
    "    print (datapath)\n",
    "    poses_path = os.path.join(datapath,'poses')\n",
    "    img_path = os.path.join(datapath,'sequences')\n",
    "    if test:\n",
    "        sequences = ['03']  #Kindly use this sequence only for testing as this has mininum number of images\n",
    "    else:\n",
    "        #Uncomment below and comment the next to next line to work with larger data \n",
    "        #sequences= ['01','03','06']\n",
    "        sequences = ['01']  \n",
    "        \n",
    "    images_set = []\n",
    "    odometry_set = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        images_set.append(torch.FloatTensor(load_images(os.path.join(img_path,sequence,'image_0'),img_size)))\n",
    "        odometry_set.append(torch.FloatTensor(load_poses(os.path.join(poses_path,sequence+'.txt'))))\n",
    "    \n",
    "    return images_set, odometry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xenial/Datasets/KITTI\n",
      "('images ', '/home/xenial/Datasets/KITTI/sequences/01/image_0')\n",
      "('images count : ', 1100)\n",
      "('pose ', '/home/xenial/Datasets/KITTI/poses/01.txt')\n",
      "('poses count: ', 1100)\n"
     ]
    }
   ],
   "source": [
    "#dataload\n",
    "X,y = VODataLoader(\"/home/xenial/Datasets/KITTI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([1100, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([1100, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Details of X :\")\n",
    "# print(type(X)) \n",
    "# print(type(X[0]))\n",
    "# print(len(X)) \n",
    "# print(len(X[0])) \n",
    "print(X[0].size())\n",
    "print(\"Details of y :\")\n",
    "# print(type(y))\n",
    "# print(type(y[0]))\n",
    "# print(len(y))\n",
    "# print(len(y[0]))\n",
    "print(y[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([110, 10, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([110, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "#Converting lists containing tensors to tensors as per the batchsize (10)\n",
    "X=torch.stack(X).view(-1,10,6, 384, 1280)\n",
    "y=torch.stack(y).view(-1,10,6)\n",
    "print(\"Details of X :\")\n",
    "print(X.size())\n",
    "print(\"Details of y :\")\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to display image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    plt.figure\n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining neural network as per RP by Sen Wang\n",
    "class DeepVONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepVONet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)) #6 64\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv3_1 = nn.Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.conv4_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.conv5_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv6 = nn.Conv2d (512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.lstm1 = nn.LSTMCell(20*6*1024, 100)\n",
    "        self.lstm2 = nn.LSTMCell(100, 100)\n",
    "        self.fc = nn.Linear(in_features=100, out_features=6)\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def reset_hidden_states(self, size=10, zero=True):\n",
    "        if zero == True:\n",
    "            self.hx1 = Variable(torch.zeros(size, 100))\n",
    "            self.cx1 = Variable(torch.zeros(size, 100))\n",
    "            self.hx2 = Variable(torch.zeros(size, 100))\n",
    "            self.cx2 = Variable(torch.zeros(size, 100))\n",
    "        else:\n",
    "            self.hx1 = Variable(self.hx1.data)\n",
    "            self.cx1 = Variable(self.cx1.data)\n",
    "            self.hx2 = Variable(self.hx2.data)\n",
    "            self.cx2 = Variable(self.cx2.data)\n",
    "\n",
    "        if next(self.parameters()).is_cuda == True:\n",
    "            self.hx1 = self.hx1.cuda()\n",
    "            self.cx1 = self.cx1.cuda()\n",
    "            self.hx2 = self.hx2.cuda()\n",
    "            self.cx2 = self.cx2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.relu3_1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.relu4_1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.relu5_1(x)\n",
    "        x = self.conv6(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0), 20 * 6 * 1024)\n",
    "        #print(x.size())\n",
    "        self.hx1, self.cx1 = self.lstm1(x, (self.hx1, self.cx1))\n",
    "        x = self.hx1\n",
    "        self.hx2, self.cx2 = self.lstm2(x, (self.hx2, self.cx2))\n",
    "        x = self.hx2\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def training_model(model, train_num, X, y, epoch_num=25):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        for i in range(train_num):\n",
    "            print(\"Train num :\", i+1)\n",
    "            inputs = X[i]\n",
    "            #print(len(inputs))\n",
    "            labels = y[i]\n",
    "            #print(len(labels))\n",
    "            model.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "            #optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if(epoch == (epoch_num-1) and (i > train_num-5)):\n",
    "                print(\"Outputs \", outputs)\n",
    "                print(\"Labels \", labels)\n",
    "            #print(outputs)\n",
    "            #print(labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch : %d Loss: %.3f' %(epoch+1, running_loss/train_num))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    print (\"Time taken in Training {0}\".format((time.time() - start_time)))\n",
    "\n",
    "#Testing functions, it is predicting the output for test sequence as per the model\n",
    "def testing_model (model, test_num, X):\n",
    "    start_time = time.time()\n",
    "    Y_output = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_num):\n",
    "            inputs = X[i]\n",
    "            outputs = model(inputs)\n",
    "            Y_output.append(outputs)\n",
    "    print (\"Time taken in Testing {0}\".format((time.time() - start_time)))\n",
    "    return torch.stack(Y_output)\n",
    "\n",
    "#Helper functions to get accuracy\n",
    "def get_accuracy(outputs, labels, batch_size):\n",
    "    diff =0\n",
    "    for i in range(batch_size):\n",
    "        for j in range(10):\n",
    "            out = outputs[j].detach().numpy()\n",
    "            lab = labels[j].detach().numpy()\n",
    "            print \"out =\\n{}\\nlab =\\n{}\".format(out, lab)\n",
    "            diff+=get_mse_diff(out,lab)\n",
    "    #print(\"Loss : \",diff/(batch_size*10),\"%\")\n",
    "    print(\"Accuracy : \",(1 -diff/(batch_size*10))*100,\"%\")\n",
    "    \n",
    "def get_mse_diff(x,y):\n",
    "    diff= 0\n",
    "    for i in range(6):\n",
    "        diff += (x[i]-y[i])*(x[i]-y[i])\n",
    "    return diff/6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVONet(\n",
      "  (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv3_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3_1): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv4_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4_1): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5_1): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (lstm1): LSTMCell(122880, 100)\n",
      "  (lstm2): LSTMCell(100, 100)\n",
      "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Creating model and defining loss and optimizer to be used \n",
    "model = DeepVONet()\n",
    "print(model)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment lines below to see model paramters \n",
    "# for parameter in model.parameters():\n",
    "#     print(len(parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch : ', 1)\n",
      "('Train num :', 1)\n",
      "('Train num :', 2)\n",
      "('Train num :', 3)\n",
      "('Train num :', 4)\n",
      "('Train num :', 5)\n",
      "('Train num :', 6)\n",
      "('Train num :', 7)\n",
      "('Outputs ', tensor([[-0.0351,  0.0246, -0.0464, -0.0082, -0.0843,  0.0168],\n",
      "        [-0.0360,  0.0240, -0.0434, -0.0141, -0.0824,  0.0175],\n",
      "        [-0.0317,  0.0310, -0.0381, -0.0093, -0.0882,  0.0202],\n",
      "        [-0.0404,  0.0177, -0.0352, -0.0091, -0.0869,  0.0198],\n",
      "        [-0.0371,  0.0256, -0.0364, -0.0098, -0.0908,  0.0233],\n",
      "        [-0.0359,  0.0236, -0.0441, -0.0034, -0.0843,  0.0196],\n",
      "        [-0.0423,  0.0275, -0.0435, -0.0037, -0.0853,  0.0195],\n",
      "        [-0.0320,  0.0280, -0.0395, -0.0075, -0.0910,  0.0163],\n",
      "        [-0.0457,  0.0279, -0.0322, -0.0033, -0.0875,  0.0242],\n",
      "        [-0.0253,  0.0277, -0.0332, -0.0123, -0.0975,  0.0202]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 1.4915e+00,  1.0313e-02, -2.3957e-01, -4.9087e-03,  8.1938e-04,\n",
      "         -4.9904e-03],\n",
      "        [ 1.5113e+00,  8.9248e-03, -2.3918e-01, -1.4378e-03,  1.7747e-03,\n",
      "         -3.4303e-04],\n",
      "        [ 1.5328e+00,  1.0299e-02, -2.4249e-01, -7.0328e-03,  4.1175e-04,\n",
      "         -7.7770e-03],\n",
      "        [ 1.5503e+00,  1.0793e-02, -2.4143e-01, -2.3696e-03,  1.0611e-03,\n",
      "         -3.3306e-03],\n",
      "        [ 1.5793e+00,  8.3756e-03, -2.4546e-01, -6.8287e-03,  2.3631e-04,\n",
      "         -7.4773e-03],\n",
      "        [ 1.5963e+00,  7.6198e-03, -2.4526e-01, -1.2417e-03,  1.1274e-03,\n",
      "         -1.2616e-03],\n",
      "        [ 1.6158e+00,  7.9395e-03, -2.4928e-01, -1.2683e-02, -2.3019e-04,\n",
      "         -1.0850e-02],\n",
      "        [ 1.6313e+00,  8.3625e-03, -2.5126e-01, -9.6938e-03, -1.8691e-04,\n",
      "         -7.8660e-03],\n",
      "        [ 1.6408e+00,  9.2725e-03, -2.5000e-01, -8.7725e-03, -2.7497e-04,\n",
      "         -9.2396e-03],\n",
      "        [ 1.6579e+00,  7.1116e-03, -2.5364e-01, -1.1672e-02, -5.8178e-04,\n",
      "         -1.3897e-02]]))\n",
      "('Train num :', 8)\n",
      "('Outputs ', tensor([[-0.0386,  0.0303, -0.0459, -0.0041, -0.0808,  0.0216],\n",
      "        [-0.0300,  0.0299, -0.0488, -0.0058, -0.0823,  0.0201],\n",
      "        [-0.0294,  0.0305, -0.0476, -0.0029, -0.0844,  0.0161],\n",
      "        [-0.0351,  0.0262, -0.0429, -0.0064, -0.0829,  0.0211],\n",
      "        [-0.0314,  0.0231, -0.0424, -0.0077, -0.0841,  0.0183],\n",
      "        [-0.0313,  0.0289, -0.0454, -0.0045, -0.0835,  0.0183],\n",
      "        [-0.0304,  0.0263, -0.0458, -0.0048, -0.0835,  0.0210],\n",
      "        [-0.0350,  0.0241, -0.0434, -0.0077, -0.0785,  0.0201],\n",
      "        [-0.0281,  0.0207, -0.0398, -0.0059, -0.0907,  0.0147],\n",
      "        [-0.0307,  0.0269, -0.0377, -0.0055, -0.0914,  0.0187]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 1.6720e+00,  3.9978e-03, -2.5017e-01,  8.4060e-04,  9.6896e-04,\n",
      "          1.0555e-03],\n",
      "        [ 1.6861e+00,  4.3992e-03, -2.5340e-01, -2.7859e-03, -7.5703e-05,\n",
      "         -2.3934e-03],\n",
      "        [ 1.7072e+00,  6.0084e-03, -2.5326e-01,  7.2444e-04,  5.9734e-04,\n",
      "          2.2204e-03],\n",
      "        [ 1.7190e+00,  8.5032e-03, -2.5799e-01, -9.2191e-03, -8.1617e-04,\n",
      "         -9.3832e-03],\n",
      "        [ 1.7362e+00,  7.3660e-03, -2.5818e-01, -2.8541e-03, -4.1043e-04,\n",
      "         -2.9919e-03],\n",
      "        [ 1.7521e+00,  7.0884e-03, -2.6154e-01, -1.1534e-03, -2.9121e-04,\n",
      "         -2.1536e-03],\n",
      "        [ 1.7624e+00,  5.2316e-03, -2.6140e-01,  4.0839e-03,  5.9762e-05,\n",
      "          3.3669e-03],\n",
      "        [ 1.7766e+00,  3.2084e-03, -2.6541e-01,  8.8202e-03, -2.1870e-04,\n",
      "          8.5228e-03],\n",
      "        [ 1.7906e+00,  1.9369e-03, -2.6930e-01,  7.2290e-03, -5.8580e-04,\n",
      "          6.4321e-03],\n",
      "        [ 1.8116e+00,  2.3264e-03, -2.7289e-01,  7.6050e-03, -1.3439e-04,\n",
      "          8.1973e-03]]))\n",
      "('Train num :', 9)\n",
      "('Outputs ', tensor([[-0.0242,  0.0245, -0.0486, -0.0062, -0.0822,  0.0165],\n",
      "        [-0.0221,  0.0250, -0.0392, -0.0051, -0.0848,  0.0154],\n",
      "        [-0.0308,  0.0285, -0.0420, -0.0089, -0.0802,  0.0181],\n",
      "        [-0.0270,  0.0263, -0.0419, -0.0064, -0.0870,  0.0163],\n",
      "        [-0.0273,  0.0233, -0.0417, -0.0146, -0.0840,  0.0166],\n",
      "        [-0.0272,  0.0264, -0.0400, -0.0095, -0.0813,  0.0146],\n",
      "        [-0.0205,  0.0334, -0.0388, -0.0093, -0.0847,  0.0187],\n",
      "        [-0.0306,  0.0281, -0.0418, -0.0080, -0.0868,  0.0179],\n",
      "        [-0.0201,  0.0311, -0.0442, -0.0085, -0.0850,  0.0169],\n",
      "        [-0.0255,  0.0239, -0.0418, -0.0066, -0.0852,  0.0190]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 1.8221e+00,  4.6053e-03, -2.7868e-01,  1.8981e-03, -9.0191e-04,\n",
      "          2.4742e-03],\n",
      "        [ 1.8391e+00,  5.6184e-03, -2.7982e-01,  1.1956e-02,  1.1843e-03,\n",
      "          1.2816e-02],\n",
      "        [ 1.8495e+00,  6.9975e-03, -2.8734e-01,  6.2859e-03, -1.0391e-03,\n",
      "          5.2510e-03],\n",
      "        [ 1.8680e+00,  6.1024e-03, -2.9192e-01,  1.4092e-02, -1.5578e-03,\n",
      "          1.2801e-02],\n",
      "        [ 1.8796e+00,  7.0715e-03, -3.0184e-01,  7.7472e-03, -3.6335e-03,\n",
      "          8.0579e-03],\n",
      "        [ 1.8901e+00,  2.0211e-03, -3.0850e-01,  2.2781e-02, -2.7473e-03,\n",
      "          2.3306e-02],\n",
      "        [ 1.8988e+00,  1.8225e-03, -3.1913e-01,  2.1796e-02, -2.9970e-03,\n",
      "          2.3013e-02],\n",
      "        [ 1.9026e+00,  1.0952e-03, -3.2997e-01,  1.7424e-02, -3.6228e-03,\n",
      "          1.6310e-02],\n",
      "        [ 1.9145e+00,  1.7025e-03, -3.4378e-01,  1.0488e-02, -4.3906e-03,\n",
      "          9.3977e-03],\n",
      "        [ 1.9212e+00, -1.9101e-03, -3.5451e-01,  1.3718e-02, -3.9533e-03,\n",
      "          1.2167e-02]]))\n",
      "('Train num :', 10)\n",
      "('Outputs ', tensor([[-0.0233,  0.0261, -0.0415, -0.0099, -0.0917,  0.0200],\n",
      "        [-0.0157,  0.0282, -0.0394, -0.0083, -0.0899,  0.0181],\n",
      "        [-0.0098,  0.0293, -0.0431, -0.0060, -0.0879,  0.0170],\n",
      "        [-0.0244,  0.0254, -0.0466, -0.0071, -0.0780,  0.0192],\n",
      "        [-0.0217,  0.0317, -0.0406, -0.0042, -0.0875,  0.0162],\n",
      "        [-0.0227,  0.0231, -0.0390, -0.0086, -0.0828,  0.0143],\n",
      "        [-0.0222,  0.0343, -0.0418, -0.0045, -0.0832,  0.0182],\n",
      "        [-0.0174,  0.0280, -0.0389, -0.0093, -0.0800,  0.0181],\n",
      "        [-0.0226,  0.0306, -0.0435, -0.0047, -0.0853,  0.0152],\n",
      "        [-0.0172,  0.0296, -0.0375, -0.0083, -0.0839,  0.0239]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 1.9338e+00, -4.4420e-03, -3.6524e-01,  1.6075e-02, -3.8106e-03,\n",
      "          1.5408e-02],\n",
      "        [ 1.9435e+00, -2.9969e-03, -3.7480e-01,  1.8679e-02, -3.5122e-03,\n",
      "          1.9902e-02],\n",
      "        [ 1.9523e+00, -1.5507e-03, -3.8496e-01,  1.8527e-02, -4.0712e-03,\n",
      "          1.8769e-02],\n",
      "        [ 1.9609e+00,  5.4060e-04, -3.9971e-01,  1.3828e-02, -4.7746e-03,\n",
      "          1.4936e-02],\n",
      "        [ 1.9663e+00,  2.2663e-03, -4.1273e-01,  1.0057e-02, -5.1099e-03,\n",
      "          1.0441e-02],\n",
      "        [ 1.9704e+00,  2.4796e-03, -4.2726e-01,  8.5307e-03, -5.3368e-03,\n",
      "          7.4504e-03],\n",
      "        [ 1.9781e+00,  2.1805e-03, -4.3865e-01,  1.1881e-02, -5.2108e-03,\n",
      "          1.1110e-02],\n",
      "        [ 1.9898e+00,  1.3127e-03, -4.5705e-01,  7.5721e-03, -5.7894e-03,\n",
      "          7.9321e-03],\n",
      "        [ 1.9956e+00,  8.9740e-04, -4.6798e-01,  1.2388e-02, -4.9478e-03,\n",
      "          1.2216e-02],\n",
      "        [ 1.9977e+00,  2.9340e-03, -4.7972e-01,  9.7771e-03, -4.7444e-03,\n",
      "          9.6074e-03]]))\n",
      "Epoch : 1 Loss: 0.365\n",
      "Finished Training\n",
      "Time taken in Training 181.986446142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_model(model,10,X,y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "#torch.save(model.state_dict(), 'DeepVO.pt')\n",
    "#Load model\n",
    "# model_loaded = torch.load('DeepVO.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xenial/Datasets/KITTI\n",
      "('images ', '/home/xenial/Datasets/KITTI/sequences/03/image_0')\n",
      "('images count : ', 800)\n",
      "('pose ', '/home/xenial/Datasets/KITTI/poses/03.txt')\n",
      "('poses count: ', 800)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 9437184000 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1511d758c8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVODataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/xenial/Datasets/KITTI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 9437184000 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = VODataLoader(\"/home/xenial/Datasets/KITTI\", test=True)\n",
    "X_test=torch.stack(X_test).view(-1,10,6, 384, 1280)\n",
    "y_test=torch.stack(y_test).view(-1,10,6)\n",
    "print(X_test.size())\n",
    "print(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions from the model \n",
    "test_batch_size = 10 #Based on this count only batches will be process, not as per the total number of batches provided\n",
    "y_output = testing_model(model,test_batch_size,X)\n",
    "print(y_output.size())\n",
    "#Saving outputs\n",
    "torch.save(y_output,\"y_output.pt\")\n",
    "#getting accuracy\n",
    "get_accuracy(y_output,y_test,test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
