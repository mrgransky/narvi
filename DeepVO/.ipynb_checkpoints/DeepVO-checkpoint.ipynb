{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get resized image from a provided path\n",
    "def get_image(path,img_size= (1280,384)):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, img_size, cv2.INTER_LINEAR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to laod images from a given directory and forming batches\n",
    "def load_images(img_dir, img_size):\n",
    "    print (\"images \", img_dir)\n",
    "    images= []\n",
    "    images_set =[]\n",
    "    for img in glob.glob(img_dir+'/*'):\n",
    "        images.append(get_image(img,img_size))\n",
    "    for i in range(len(images)-1):\n",
    "        img1 = images[i]\n",
    "        img2 = images[i+1]\n",
    "        img = np.concatenate([img1, img2],axis = -1)\n",
    "        images_set.append(img)\n",
    "    print(\"images count : \",len(images_set))\n",
    "    images_set = np.reshape(images_set, (-1, 6, 384, 1280))\n",
    "    return images_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for pose preprocessing \n",
    "def isRotationMatrix(R):\n",
    "    \"\"\" Checks if a matrix is a valid rotation matrix\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "    \"\"\"\n",
    "    Rt = np.transpose(R)\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    return n < 1e-6\n",
    "\n",
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\" calculates rotation matrix to euler angles\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles\n",
    "    \"\"\"\n",
    "    assert(isRotationMatrix(R))\n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "def getMatrices(all_poses):\n",
    "    all_matrices = []\n",
    "    for i in range(len(all_poses)):\n",
    "        #print(\"I: \",i)\n",
    "        j = all_poses[i]\n",
    "        #print(\"J:   \",j)\n",
    "        p = np.array([j[3], j[7], j[11]])\n",
    "        #print(\"P:   \", p)\n",
    "        R = np.array([[j[0],j[1],j[2]],\n",
    "                [j[4],j[5],j[6]],\n",
    "                [j[8],j[9],j[10]]])\n",
    "        #print(\"R:   \", R)\n",
    "        angles = rotationMatrixToEulerAngles(R)\n",
    "        #print(\"Angles: \",angles)\n",
    "        matrix = np.concatenate((p,angles))\n",
    "        #print(\"MATRIX: \", matrix)\n",
    "        all_matrices.append(matrix)\n",
    "    return all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to get poses form a given location \n",
    "def load_poses(pose_file):\n",
    "    print (\"pose \",pose_file)\n",
    "    poses = []\n",
    "    poses_set = []\n",
    "    with open(pose_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose = np.fromstring(line, dtype=float, sep=' ')\n",
    "            poses.append(pose)\n",
    "    poses = getMatrices(poses)\n",
    "    for i in range(len(poses)-1):\n",
    "        pose1 = poses[i]\n",
    "        pose2 = poses[i+1]\n",
    "        finalpose = pose2-pose1\n",
    "        poses_set.append(finalpose)\n",
    "    print(\"poses count: \",len(poses_set))\n",
    "    return poses_set          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary dataloader function to get both images and poses\n",
    "def VODataLoader(datapath,img_size= (1280,384), test=False):\n",
    "    print (datapath)\n",
    "    poses_path = os.path.join(datapath,'poses')\n",
    "    img_path = os.path.join(datapath,'sequences')\n",
    "    if test:\n",
    "        sequences = ['03']  #Kindly use this sequence only for testing as this has mininum number of images\n",
    "    else:\n",
    "        #Uncomment below and comment the next to next line to work with larger data \n",
    "        #sequences= ['01','03','06']\n",
    "        sequences = ['03']  \n",
    "        \n",
    "    images_set = []\n",
    "    odometry_set = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        images_set.append(torch.FloatTensor(load_images(os.path.join(img_path,sequence,'image_0'),img_size)))\n",
    "        odometry_set.append(torch.FloatTensor(load_poses(os.path.join(poses_path,sequence+'.txt'))))\n",
    "    \n",
    "    return images_set, odometry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xenial/Datasets/KITTI\n",
      "('images ', '/home/xenial/Datasets/KITTI/sequences/03/image_0')\n",
      "('images count : ', 800)\n",
      "('pose ', '/home/xenial/Datasets/KITTI/poses/03.txt')\n",
      "('poses count: ', 800)\n"
     ]
    }
   ],
   "source": [
    "#dataload\n",
    "X,y = VODataLoader(\"/home/xenial/Datasets/KITTI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([800, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([800, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Details of X :\")\n",
    "# print(type(X)) \n",
    "# print(type(X[0]))\n",
    "# print(len(X)) \n",
    "# print(len(X[0])) \n",
    "print(X[0].size())\n",
    "print(\"Details of y :\")\n",
    "# print(type(y))\n",
    "# print(type(y[0]))\n",
    "# print(len(y))\n",
    "# print(len(y[0]))\n",
    "print(y[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([80, 10, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([80, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "#Converting lists containing tensors to tensors as per the batchsize (10)\n",
    "X=torch.stack(X).view(-1,10,6, 384, 1280)\n",
    "y=torch.stack(y).view(-1,10,6)\n",
    "print(\"Details of X :\")\n",
    "print(X.size())\n",
    "print(\"Details of y :\")\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to display image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    plt.figure\n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining neural network as per RP by Sen Wang\n",
    "class DeepVONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepVONet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)) #6 64\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv3_1 = nn.Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.conv4_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.conv5_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv6 = nn.Conv2d (512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.lstm1 = nn.LSTMCell(20*6*1024, 100)\n",
    "        self.lstm2 = nn.LSTMCell(100, 100)\n",
    "        self.fc = nn.Linear(in_features=100, out_features=6)\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def reset_hidden_states(self, size=10, zero=True):\n",
    "        if zero == True:\n",
    "            self.hx1 = Variable(torch.zeros(size, 100))\n",
    "            self.cx1 = Variable(torch.zeros(size, 100))\n",
    "            self.hx2 = Variable(torch.zeros(size, 100))\n",
    "            self.cx2 = Variable(torch.zeros(size, 100))\n",
    "        else:\n",
    "            self.hx1 = Variable(self.hx1.data)\n",
    "            self.cx1 = Variable(self.cx1.data)\n",
    "            self.hx2 = Variable(self.hx2.data)\n",
    "            self.cx2 = Variable(self.cx2.data)\n",
    "\n",
    "        if next(self.parameters()).is_cuda == True:\n",
    "            self.hx1 = self.hx1.cuda()\n",
    "            self.cx1 = self.cx1.cuda()\n",
    "            self.hx2 = self.hx2.cuda()\n",
    "            self.cx2 = self.cx2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.relu3_1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.relu4_1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.relu5_1(x)\n",
    "        x = self.conv6(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0), 20 * 6 * 1024)\n",
    "        #print(x.size())\n",
    "        self.hx1, self.cx1 = self.lstm1(x, (self.hx1, self.cx1))\n",
    "        x = self.hx1\n",
    "        self.hx2, self.cx2 = self.lstm2(x, (self.hx2, self.cx2))\n",
    "        x = self.hx2\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def training_model(model, train_num, X, y, epoch_num=25):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        for i in range(train_num):\n",
    "            print(\"Train num :\", i+1)\n",
    "            inputs = X[i]\n",
    "            #print(len(inputs))\n",
    "            labels = y[i]\n",
    "            #print(len(labels))\n",
    "            model.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "            #optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if(epoch == (epoch_num-1) and (i > train_num-5)):\n",
    "                print(\"Outputs \", outputs)\n",
    "                print(\"Labels \", labels)\n",
    "            #print(outputs)\n",
    "            #print(labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch : %d Loss: %.3f' %(epoch+1, running_loss/train_num))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    print (\"Time taken in Training {0}\".format((time.time() - start_time)))\n",
    "\n",
    "#Testing functions, it is predicting the output for test sequence as per the model\n",
    "def testing_model (model, test_num, X):\n",
    "    start_time = time.time()\n",
    "    Y_output = []\n",
    "    count = 0\n",
    "    totcount = 0\n",
    "    for i in range(test_num):\n",
    "            # get the inputs\n",
    "            inputs = X[i]\n",
    "            outputs = model(inputs)\n",
    "            Y_output.append(outputs)\n",
    "    print (\"Time taken in Testing {0}\".format((time.time() - start_time)))\n",
    "    return torch.stack(Y_output)\n",
    "\n",
    "#Helper functions to get accuracy\n",
    "def get_accuracy(outputs, labels, batch_size):\n",
    "    diff =0\n",
    "    for i in range(batch_size):\n",
    "        for j in range(10):\n",
    "            print \"\\n\\n#################################################n\\n\"\n",
    "            print \"\\n\\n(i,j) = \\t \", i , \" , \", j\n",
    "            out = outputs[j].detach().numpy()\n",
    "            print \"\\nout[\", j,\"] = \", out\n",
    "            lab = labels[j].detach().numpy()\n",
    "            print \"\\nlab[\", j,\"] = \", lab\n",
    "            diff+= get_mse_diff(out,lab)\n",
    "            print \"\\ndiff = \", diff\n",
    "            print \"\\n\\n------------------------------------------------\\n\\n\"\n",
    "    print \"Loss : \", diff/(batch_size*10) ,\" %\"\n",
    "    print \"Accuracy : \",(1 -diff/(batch_size*10))*100, \" %\"\n",
    "    \n",
    "def get_mse_diff(x,y):\n",
    "    diff= 0\n",
    "    for i in range(6):\n",
    "        diff += (x[i]-y[i])*(x[i]-y[i])\n",
    "    return diff/6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVONet(\n",
      "  (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv3_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3_1): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv4_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4_1): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5_1): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (lstm1): LSTMCell(122880, 100)\n",
      "  (lstm2): LSTMCell(100, 100)\n",
      "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Creating model and defining loss and optimizer to be used \n",
    "model = DeepVONet()\n",
    "print(model)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "128\n",
      "128\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "1024\n",
      "1024\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#Uncomment lines below to see model paramters \n",
    "for parameter in model.parameters():\n",
    "    print(len(parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch : ', 1)\n",
      "('Train num :', 1)\n",
      "('Train num :', 2)\n",
      "('Train num :', 3)\n",
      "('Train num :', 4)\n",
      "('Train num :', 5)\n",
      "('Train num :', 6)\n",
      "('Train num :', 7)\n",
      "('Train num :', 8)\n",
      "('Train num :', 9)\n",
      "('Train num :', 10)\n",
      "Epoch : 1 Loss: 0.075\n",
      "('Epoch : ', 2)\n",
      "('Train num :', 1)\n",
      "('Train num :', 2)\n",
      "('Train num :', 3)\n",
      "('Train num :', 4)\n",
      "('Train num :', 5)\n",
      "('Train num :', 6)\n",
      "('Train num :', 7)\n",
      "('Outputs ', tensor([[-0.0853,  0.0534,  0.0377,  0.0421, -0.0673,  0.0343],\n",
      "        [-0.0865,  0.0558,  0.0455,  0.0430, -0.0687,  0.0278],\n",
      "        [-0.0914,  0.0585,  0.0362,  0.0499, -0.0610,  0.0315],\n",
      "        [-0.0839,  0.0637,  0.0390,  0.0473, -0.0644,  0.0345],\n",
      "        [-0.0874,  0.0590,  0.0444,  0.0440, -0.0624,  0.0322],\n",
      "        [-0.0844,  0.0655,  0.0402,  0.0464, -0.0655,  0.0368],\n",
      "        [-0.0863,  0.0597,  0.0426,  0.0441, -0.0619,  0.0406],\n",
      "        [-0.0871,  0.0604,  0.0418,  0.0459, -0.0648,  0.0323],\n",
      "        [-0.0863,  0.0525,  0.0349,  0.0443, -0.0650,  0.0292],\n",
      "        [-0.0843,  0.0579,  0.0373,  0.0434, -0.0572,  0.0305]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 2.0594e-02, -1.4955e-03,  3.9705e-01, -1.7261e-03,  7.7224e-03,\n",
      "          3.9439e-04],\n",
      "        [ 2.0952e-02,  3.5090e-04,  3.7189e-01, -1.3596e-03,  8.0403e-03,\n",
      "         -1.8822e-03],\n",
      "        [ 2.0416e-02,  4.2220e-04,  3.6276e-01, -6.1519e-04,  9.1312e-03,\n",
      "         -3.5769e-03],\n",
      "        [ 1.8255e-02, -1.6629e-03,  3.1108e-01, -3.1642e-04,  8.4387e-03,\n",
      "         -4.7554e-03],\n",
      "        [ 2.3354e-02, -3.9830e-03,  2.9824e-01, -9.7984e-04,  9.0263e-03,\n",
      "         -3.7545e-03],\n",
      "        [ 2.9870e-02, -8.8722e-03,  2.7424e-01, -9.2934e-04,  9.3049e-03,\n",
      "          3.6638e-04],\n",
      "        [ 3.3167e-02, -7.5066e-03,  2.6568e-01,  1.4069e-03,  1.0867e-02,\n",
      "          1.0532e-03],\n",
      "        [ 3.6141e-02, -6.8121e-03,  2.5903e-01,  4.1413e-03,  1.2283e-02,\n",
      "          1.5869e-03],\n",
      "        [ 3.9209e-02, -5.8532e-03,  2.5680e-01,  3.4017e-03,  1.3540e-02,\n",
      "         -6.4545e-04],\n",
      "        [ 4.1005e-02, -7.1255e-03,  2.5469e-01,  7.3729e-04,  1.5821e-02,\n",
      "         -1.7898e-03]]))\n",
      "('Train num :', 8)\n",
      "('Outputs ', tensor([[-0.0856,  0.0573,  0.0378,  0.0406, -0.0678,  0.0420],\n",
      "        [-0.0880,  0.0580,  0.0521,  0.0452, -0.0604,  0.0369],\n",
      "        [-0.0775,  0.0647,  0.0392,  0.0465, -0.0678,  0.0391],\n",
      "        [-0.0843,  0.0634,  0.0375,  0.0415, -0.0671,  0.0387],\n",
      "        [-0.0829,  0.0593,  0.0373,  0.0502, -0.0650,  0.0375],\n",
      "        [-0.0815,  0.0556,  0.0423,  0.0424, -0.0725,  0.0390],\n",
      "        [-0.0862,  0.0589,  0.0404,  0.0436, -0.0661,  0.0402],\n",
      "        [-0.0845,  0.0639,  0.0428,  0.0516, -0.0656,  0.0376],\n",
      "        [-0.0890,  0.0611,  0.0331,  0.0483, -0.0665,  0.0314],\n",
      "        [-0.0895,  0.0517,  0.0379,  0.0453, -0.0699,  0.0346]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 4.8311e-02, -5.5242e-03,  2.6262e-01, -1.3854e-04,  1.7194e-02,\n",
      "         -2.0468e-03],\n",
      "        [ 5.0719e-02, -5.4792e-03,  2.3645e-01,  1.0290e-04,  1.6465e-02,\n",
      "         -4.3896e-04],\n",
      "        [ 6.0554e-02, -5.1905e-03,  2.4443e-01,  1.1376e-04,  1.8530e-02,\n",
      "          7.0562e-04],\n",
      "        [ 6.7503e-02, -5.5289e-03,  2.4675e-01, -7.2085e-04,  2.0364e-02,\n",
      "          7.1496e-04],\n",
      "        [ 6.7030e-02, -4.4935e-03,  2.2994e-01, -1.2318e-04,  2.0068e-02,\n",
      "         -5.8158e-04],\n",
      "        [ 7.3704e-02, -4.9219e-03,  2.3803e-01,  5.3368e-04,  2.1982e-02,\n",
      "         -2.0472e-03],\n",
      "        [ 8.1427e-02, -6.2023e-03,  2.3619e-01,  2.9898e-04,  2.3597e-02,\n",
      "         -1.3084e-03],\n",
      "        [ 9.0274e-02, -7.5956e-03,  2.3871e-01, -1.1262e-03,  2.5586e-02,\n",
      "         -1.6708e-03],\n",
      "        [ 9.5974e-02, -8.5755e-03,  2.3910e-01, -2.0531e-03,  2.7147e-02,\n",
      "         -3.6824e-03],\n",
      "        [ 1.0653e-01, -9.8741e-03,  2.4145e-01, -1.9572e-03,  2.9621e-02,\n",
      "         -3.2723e-03]]))\n",
      "('Train num :', 9)\n",
      "('Outputs ', tensor([[-0.0820,  0.0580,  0.0380,  0.0490, -0.0620,  0.0363],\n",
      "        [-0.0863,  0.0539,  0.0406,  0.0412, -0.0646,  0.0356],\n",
      "        [-0.0863,  0.0612,  0.0370,  0.0485, -0.0645,  0.0396],\n",
      "        [-0.0867,  0.0598,  0.0399,  0.0439, -0.0635,  0.0340],\n",
      "        [-0.0840,  0.0580,  0.0423,  0.0460, -0.0586,  0.0337],\n",
      "        [-0.0870,  0.0584,  0.0426,  0.0374, -0.0651,  0.0314],\n",
      "        [-0.0870,  0.0575,  0.0426,  0.0533, -0.0642,  0.0341],\n",
      "        [-0.0887,  0.0593,  0.0391,  0.0486, -0.0617,  0.0334],\n",
      "        [-0.0822,  0.0659,  0.0372,  0.0489, -0.0631,  0.0415],\n",
      "        [-0.0896,  0.0622,  0.0444,  0.0474, -0.0641,  0.0366]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 0.1150, -0.0105,  0.2291, -0.0014,  0.0303, -0.0014],\n",
      "        [ 0.1307, -0.0098,  0.2261,  0.0005,  0.0326,  0.0018],\n",
      "        [ 0.1451, -0.0091,  0.2245,  0.0018,  0.0362,  0.0031],\n",
      "        [ 0.1519, -0.0090,  0.2199,  0.0026,  0.0379,  0.0012],\n",
      "        [ 0.1628, -0.0095,  0.2187,  0.0023,  0.0397, -0.0009],\n",
      "        [ 0.1793, -0.0107,  0.2251,  0.0010,  0.0418, -0.0018],\n",
      "        [ 0.1951, -0.0122,  0.2287, -0.0011,  0.0439, -0.0039],\n",
      "        [ 0.2071, -0.0155,  0.2220, -0.0040,  0.0449, -0.0067],\n",
      "        [ 0.2294, -0.0165,  0.2320, -0.0054,  0.0466, -0.0082],\n",
      "        [ 0.2566, -0.0180,  0.2266, -0.0030,  0.0492, -0.0036]]))\n",
      "('Train num :', 10)\n",
      "('Outputs ', tensor([[-0.0885,  0.0600,  0.0393,  0.0450, -0.0684,  0.0289],\n",
      "        [-0.0808,  0.0562,  0.0405,  0.0449, -0.0624,  0.0316],\n",
      "        [-0.0771,  0.0564,  0.0402,  0.0502, -0.0649,  0.0468],\n",
      "        [-0.0788,  0.0633,  0.0398,  0.0506, -0.0653,  0.0410],\n",
      "        [-0.0842,  0.0645,  0.0339,  0.0492, -0.0621,  0.0330],\n",
      "        [-0.0801,  0.0604,  0.0378,  0.0513, -0.0635,  0.0299],\n",
      "        [-0.0877,  0.0573,  0.0382,  0.0364, -0.0629,  0.0290],\n",
      "        [-0.0792,  0.0638,  0.0355,  0.0479, -0.0642,  0.0273],\n",
      "        [-0.0837,  0.0543,  0.0386,  0.0423, -0.0640,  0.0384],\n",
      "        [-0.0899,  0.0626,  0.0557,  0.0397, -0.0610,  0.0279]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 2.8782e-01, -1.2927e-02,  2.1552e-01, -3.1928e-03,  5.1621e-02,\n",
      "         -5.8455e-04],\n",
      "        [ 3.2365e-01, -1.2920e-04,  2.1624e-01, -2.6275e-03,  5.2530e-02,\n",
      "         -1.3384e-03],\n",
      "        [ 3.4295e-01, -5.4504e-03,  2.0469e-01, -4.2049e-03,  5.5079e-02,\n",
      "         -3.8064e-03],\n",
      "        [ 3.6108e-01,  3.9437e-03,  1.9395e-01, -1.3758e-03,  5.5929e-02,\n",
      "         -1.8820e-03],\n",
      "        [ 3.8550e-01, -6.8134e-03,  1.8574e-01, -5.8764e-03,  5.6795e-02,\n",
      "         -4.8008e-03],\n",
      "        [ 4.0688e-01, -5.3029e-03,  1.6584e-01, -9.7603e-03,  5.5622e-02,\n",
      "         -7.4900e-03],\n",
      "        [ 4.3222e-01, -2.5228e-02,  1.4855e-01, -1.6694e-02,  5.5205e-02,\n",
      "         -1.4350e-02],\n",
      "        [ 4.4306e-01, -2.2998e-02,  1.3449e-01, -2.7119e-02,  5.3026e-02,\n",
      "         -2.6305e-02],\n",
      "        [ 4.6230e-01, -2.4975e-02,  1.0298e-01, -1.4363e-02,  5.2293e-02,\n",
      "         -1.6000e-02],\n",
      "        [ 4.8182e-01, -3.1181e-02,  8.0840e-02,  6.1805e-03,  5.0038e-02,\n",
      "          8.7695e-03]]))\n",
      "Epoch : 2 Loss: 0.070\n",
      "Finished Training\n",
      "Time taken in Training 200.93055892\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_model(model,10,X,y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(), 'DeepVO.pt')\n",
    "\n",
    "#Load model\n",
    "model_loaded = torch.load('DeepVO.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xenial/Datasets/KITTI\n",
      "('images ', '/home/xenial/Datasets/KITTI/sequences/03/image_0')\n",
      "('images count : ', 800)\n",
      "('pose ', '/home/xenial/Datasets/KITTI/poses/03.txt')\n",
      "('poses count: ', 800)\n",
      "torch.Size([80, 10, 6, 384, 1280])\n",
      "torch.Size([80, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = VODataLoader(\"/home/xenial/Datasets/KITTI\", test=True)\n",
    "X_test=torch.stack(X_test).view(-1,10,6, 384, 1280)\n",
    "y_test=torch.stack(y_test).view(-1,10,6)\n",
    "print(X_test.size())\n",
    "print(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in Testing 5.08949899673\n",
      "y_output size =  torch.Size([2, 10, 6])\n",
      "y_output shape =  2\n",
      "\n",
      "\n",
      "#################################################n\n",
      "\n",
      "\n",
      "\n",
      "(i,j) = \t  0  ,  0\n",
      "\n",
      "out[ 0 ] =  [[-0.07498735  0.05976174  0.07322549  0.06136061 -0.05959743  0.03039489]\n",
      " [-0.06514043  0.05753057  0.07077852  0.05564113 -0.05648251  0.03177363]\n",
      " [-0.06464832  0.05783165  0.07672159  0.0614413  -0.06184073  0.04866632]\n",
      " [-0.05894465  0.06385045  0.06369621  0.06647249 -0.06294997  0.05359197]\n",
      " [-0.07025081  0.0608654   0.06488691  0.07361326 -0.06241847  0.03999723]\n",
      " [-0.06694921  0.06266665  0.07883155  0.06640925 -0.05738878  0.02678783]\n",
      " [-0.0767032   0.0575252   0.06416459  0.04832728 -0.05535987  0.0297574 ]\n",
      " [-0.06920993  0.06442434  0.07123785  0.06037706 -0.05926078  0.02860413]\n",
      " [-0.06260289  0.06530894  0.06968562  0.06509724 -0.05899714  0.04446422]\n",
      " [-0.07611938  0.06379888  0.08283225  0.05115207 -0.05846725  0.02797664]]\n",
      "\n",
      "lab[ 0 ] =  [[-1.1011020e-02 -1.7121639e-02  9.6171302e-01  4.3259654e-03\n",
      "   1.5365811e-03 -7.0557266e-04]\n",
      " [-5.9842700e-03 -1.5919941e-02  9.6317202e-01  9.0029015e-04\n",
      "   5.8542698e-04  2.1006465e-03]\n",
      " [-1.4071730e-02 -3.0376950e-02  9.6316898e-01 -4.3087453e-03\n",
      "   7.2347117e-05  1.4821582e-03]\n",
      " [-1.3560360e-02 -3.1617679e-02  9.6025801e-01 -6.4576236e-03\n",
      "  -7.4317917e-04  1.4930688e-03]\n",
      " [-8.7450398e-03 -2.7181091e-02  9.5823902e-01  8.8113366e-04\n",
      "  -1.3608676e-03  4.4302340e-03]\n",
      " [-1.1213070e-02 -2.2653200e-02  9.3240702e-01  3.0393933e-03\n",
      "  -1.0645633e-03  2.7425375e-03]\n",
      " [-1.0842430e-02 -8.6968001e-03  9.3743700e-01  3.0610515e-03\n",
      "  -8.3528261e-04  5.6314346e-04]\n",
      " [-1.3936370e-02 -1.4314100e-02  9.2976201e-01  6.4420595e-04\n",
      "  -8.1077503e-04 -6.2879390e-04]\n",
      " [-1.1939110e-02 -1.1284700e-02  9.2932898e-01 -2.3155636e-03\n",
      "  -9.2442252e-04 -6.5181172e-04]\n",
      " [-1.2828300e-02 -7.4187000e-03  9.3038201e-01 -2.9347069e-03\n",
      "  -6.7440607e-04 -6.5716165e-05]]\n",
      "\n",
      "diff =  [0.00318327 0.00720544 0.78367615 0.00419951 0.00359958 0.00143859]\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#################################################n\n",
      "\n",
      "\n",
      "\n",
      "(i,j) = \t  0  ,  1\n",
      "\n",
      "out[ 1 ] =  [[-0.06400093  0.07084555  0.10073075  0.07005948 -0.05344023  0.03099659]\n",
      " [-0.05126901  0.06299023  0.08606101  0.06666376 -0.05394891  0.0345355 ]\n",
      " [-0.05263665  0.06678471  0.09170366  0.06850226 -0.05448508  0.04694776]\n",
      " [-0.04788591  0.06725522  0.08026082  0.07488947 -0.06051651  0.05419753]\n",
      " [-0.05593653  0.0669209   0.08264527  0.08391804 -0.05757338  0.03963446]\n",
      " [-0.05811145  0.06670745  0.0929136   0.07160256 -0.05384333  0.03155266]\n",
      " [-0.05696479  0.07638814  0.0846951   0.05412619 -0.05174224  0.03065333]\n",
      " [-0.05449013  0.06749853  0.09042513  0.07123341 -0.05772388  0.02981925]\n",
      " [-0.04773182  0.0801771   0.08457831  0.07541452 -0.05809319  0.05006541]\n",
      " [-0.05687358  0.07220547  0.09068732  0.06744424 -0.05416431  0.03038845]]\n",
      "\n",
      "lab[ 1 ] =  [[-1.3425700e-02 -5.0452999e-03  9.2305201e-01 -1.3209335e-03\n",
      "  -9.8675909e-04  1.6869247e-04]\n",
      " [-1.4637100e-02 -8.0883997e-03  9.5060003e-01 -6.7170878e-04\n",
      "  -5.9826497e-04  1.5648196e-03]\n",
      " [-1.2686800e-02 -1.5841600e-02  8.8393003e-01  9.7876575e-05\n",
      "  -9.5865707e-04  1.7200297e-04]\n",
      " [-1.4431000e-02 -1.9925701e-02  9.0979999e-01  1.5681669e-03\n",
      "  -6.9372944e-04  2.3342784e-04]\n",
      " [-1.5594000e-02 -2.4766400e-02  9.0811002e-01  2.1926272e-03\n",
      "  -1.5615007e-03  4.7902600e-04]\n",
      " [-1.4378300e-02 -1.5805200e-02  9.0416002e-01  1.2474577e-03\n",
      "  -1.5025112e-03  1.3101884e-03]\n",
      " [-1.7663701e-02 -9.0531996e-03  9.0074998e-01  5.3814537e-04\n",
      "  -1.6742182e-03 -3.6678647e-04]\n",
      " [-1.9764701e-02 -8.2504004e-03  8.9682001e-01  9.0458756e-04\n",
      "  -1.4617516e-03 -1.2984625e-03]\n",
      " [-1.8139100e-02 -4.8829000e-03  8.9174998e-01  6.4142456e-05\n",
      "  -1.7232217e-03 -1.4484251e-03]\n",
      " [-1.8225800e-02 -6.1615999e-03  8.9200002e-01 -9.8225533e-04\n",
      "  -1.5465027e-03  5.5454811e-04]]\n",
      "\n",
      "diff =  [0.00487578 0.01394779 1.4634945  0.00941839 0.00658598 0.00303614]\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#################################################n\n",
      "\n",
      "\n",
      "\n",
      "(i,j) = \t  0  ,  2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ce605662aaa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#getting accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-cbbef24a479b>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(outputs, labels, batch_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n\\n#################################################n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n\\n(i,j) = \\t \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\" , \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\nout[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"] = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "#Getting predictions from the model \n",
    "test_batch_size = 2 \n",
    "\n",
    "y_output = testing_model(model, test_batch_size, X)\n",
    "\n",
    "print \"y_output size = \" , y_output.size()\n",
    "print \"y_output shape = \" , y_output.shape[0]\n",
    "\n",
    "torch.save(y_output,\"y_output.pt\")\n",
    "print \"output saved successfully!\"\n",
    "\n",
    "#getting accuracy\n",
    "get_accuracy(y_output, y_test, test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
