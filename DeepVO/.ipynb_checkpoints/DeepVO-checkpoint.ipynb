{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get resized image from a provided path\n",
    "def get_image(path,img_size= (1280,384)):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, img_size, cv2.INTER_LINEAR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to laod images from a given directory and forming batches\n",
    "def load_images(img_dir, img_size):\n",
    "    print (\"images \", img_dir)\n",
    "    images= []\n",
    "    images_set =[]\n",
    "    for img in glob.glob(img_dir+'/*'):\n",
    "        images.append(get_image(img,img_size))\n",
    "    for i in range(len(images)-1):\n",
    "        img1 = images[i]\n",
    "        img2 = images[i+1]\n",
    "        img = np.concatenate([img1, img2],axis = -1)\n",
    "        images_set.append(img)\n",
    "    print(\"images count : \",len(images_set))\n",
    "    images_set = np.reshape(images_set, (-1, 6, 384, 1280))\n",
    "    return images_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for pose preprocessing \n",
    "def isRotationMatrix(R):\n",
    "    \"\"\" Checks if a matrix is a valid rotation matrix\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "    \"\"\"\n",
    "    Rt = np.transpose(R)\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    return n < 1e-6\n",
    "\n",
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\" calculates rotation matrix to euler angles\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles\n",
    "    \"\"\"\n",
    "    assert(isRotationMatrix(R))\n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "def getMatrices(all_poses):\n",
    "    all_matrices = []\n",
    "    for i in range(len(all_poses)):\n",
    "        #print(\"I: \",i)\n",
    "        j = all_poses[i]\n",
    "        #print(\"J:   \",j)\n",
    "        p = np.array([j[3], j[7], j[11]])\n",
    "        #print(\"P:   \", p)\n",
    "        R = np.array([[j[0],j[1],j[2]],\n",
    "                [j[4],j[5],j[6]],\n",
    "                [j[8],j[9],j[10]]])\n",
    "        #print(\"R:   \", R)\n",
    "        angles = rotationMatrixToEulerAngles(R)\n",
    "        #print(\"Angles: \",angles)\n",
    "        matrix = np.concatenate((p,angles))\n",
    "        #print(\"MATRIX: \", matrix)\n",
    "        all_matrices.append(matrix)\n",
    "    return all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to get poses form a given location \n",
    "def load_poses(pose_file):\n",
    "    print (\"pose \",pose_file)\n",
    "    poses = []\n",
    "    poses_set = []\n",
    "    with open(pose_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose = np.fromstring(line, dtype=float, sep=' ')\n",
    "            poses.append(pose)\n",
    "    poses = getMatrices(poses)\n",
    "    for i in range(len(poses)-1):\n",
    "        pose1 = poses[i]\n",
    "        pose2 = poses[i+1]\n",
    "        finalpose = pose2-pose1\n",
    "        poses_set.append(finalpose)\n",
    "    print(\"poses count: \",len(poses_set))\n",
    "    return poses_set          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary dataloader function to get both images and poses\n",
    "def VODataLoader(datapath,img_size= (1280,384), test=False):\n",
    "    print (datapath)\n",
    "    poses_path = os.path.join(datapath,'poses')\n",
    "    img_path = os.path.join(datapath,'sequences')\n",
    "    if test:\n",
    "        sequences = ['03']  #Kindly use this sequence only for testing as this has mininum number of images\n",
    "    else:\n",
    "        #Uncomment below and comment the next to next line to work with larger data \n",
    "        #sequences= ['01','03','06']\n",
    "        sequences = ['03']  \n",
    "        \n",
    "    images_set = []\n",
    "    odometry_set = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        images_set.append(torch.FloatTensor(load_images(os.path.join(img_path,sequence,'image_0'),img_size)))\n",
    "        odometry_set.append(torch.FloatTensor(load_poses(os.path.join(poses_path,sequence+'.txt'))))\n",
    "    \n",
    "    return images_set, odometry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xenial/Datasets/KITTI\n",
      "('images ', '/home/xenial/Datasets/KITTI/sequences/03/image_0')\n",
      "('images count : ', 800)\n",
      "('pose ', '/home/xenial/Datasets/KITTI/poses/03.txt')\n",
      "('poses count: ', 800)\n"
     ]
    }
   ],
   "source": [
    "#dataload\n",
    "X,y = VODataLoader(\"/home/xenial/Datasets/KITTI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([800, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([800, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Details of X :\")\n",
    "# print(type(X)) \n",
    "# print(type(X[0]))\n",
    "# print(len(X)) \n",
    "# print(len(X[0])) \n",
    "print(X[0].size())\n",
    "print(\"Details of y :\")\n",
    "# print(type(y))\n",
    "# print(type(y[0]))\n",
    "# print(len(y))\n",
    "# print(len(y[0]))\n",
    "print(y[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([80, 10, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([80, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "#Converting lists containing tensors to tensors as per the batchsize (10)\n",
    "X=torch.stack(X).view(-1,10,6, 384, 1280)\n",
    "y=torch.stack(y).view(-1,10,6)\n",
    "print(\"Details of X :\")\n",
    "print(X.size())\n",
    "print(\"Details of y :\")\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to display image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    plt.figure\n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining neural network as per RP by Sen Wang\n",
    "class DeepVONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepVONet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)) #6 64\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv3_1 = nn.Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.conv4_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.conv5_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv6 = nn.Conv2d (512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.lstm1 = nn.LSTMCell(20*6*1024, 100)\n",
    "        self.lstm2 = nn.LSTMCell(100, 100)\n",
    "        self.fc = nn.Linear(in_features=100, out_features=6)\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def reset_hidden_states(self, size=10, zero=True):\n",
    "        if zero == True:\n",
    "            self.hx1 = Variable(torch.zeros(size, 100))\n",
    "            self.cx1 = Variable(torch.zeros(size, 100))\n",
    "            self.hx2 = Variable(torch.zeros(size, 100))\n",
    "            self.cx2 = Variable(torch.zeros(size, 100))\n",
    "        else:\n",
    "            self.hx1 = Variable(self.hx1.data)\n",
    "            self.cx1 = Variable(self.cx1.data)\n",
    "            self.hx2 = Variable(self.hx2.data)\n",
    "            self.cx2 = Variable(self.cx2.data)\n",
    "\n",
    "        if next(self.parameters()).is_cuda == True:\n",
    "            self.hx1 = self.hx1.cuda()\n",
    "            self.cx1 = self.cx1.cuda()\n",
    "            self.hx2 = self.hx2.cuda()\n",
    "            self.cx2 = self.cx2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.relu3_1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.relu4_1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.relu5_1(x)\n",
    "        x = self.conv6(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0), 20 * 6 * 1024)\n",
    "        #print(x.size())\n",
    "        self.hx1, self.cx1 = self.lstm1(x, (self.hx1, self.cx1))\n",
    "        x = self.hx1\n",
    "        self.hx2, self.cx2 = self.lstm2(x, (self.hx2, self.cx2))\n",
    "        x = self.hx2\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def training_model(model, train_num, X, y, epoch_num=25):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        for i in range(train_num):\n",
    "            print(\"Train num :\", i+1)\n",
    "            inputs = X[i]\n",
    "            #print(len(inputs))\n",
    "            labels = y[i]\n",
    "            #print(len(labels))\n",
    "            model.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "            #optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if(epoch == (epoch_num-1) and (i > train_num-5)):\n",
    "                print(\"Outputs \", outputs)\n",
    "                print(\"Labels \", labels)\n",
    "            #print(outputs)\n",
    "            #print(labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch : %d Loss: %.3f' %(epoch+1, running_loss/train_num))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    print (\"Time taken in Training {0}\".format((time.time() - start_time)))\n",
    "\n",
    "#Testing functions, it is predicting the output for test sequence as per the model\n",
    "def testing_model (model, test_num, X):\n",
    "    start_time = time.time()\n",
    "    Y_output = []\n",
    "    count = 0\n",
    "    totcount = 0\n",
    "    for i in range(test_num):\n",
    "            # get the inputs\n",
    "            inputs = X[i]\n",
    "            outputs = model(inputs)\n",
    "            Y_output.append(outputs)\n",
    "    print (\"Time taken in Testing {0}\".format((time.time() - start_time)))\n",
    "    return torch.stack(Y_output)\n",
    "\n",
    "#Helper functions to get accuracy\n",
    "def get_accuracy(outputs, labels, batch_size):\n",
    "    diff =0\n",
    "    for i in range(batch_size):\n",
    "        for j in range(10):\n",
    "            print \"\\n\\n#################################################n\\n\"\n",
    "            print \"\\n\\n(i,j) = \\t \", i , \" , \", j\n",
    "            out = outputs[j].detach().numpy()\n",
    "            print \"\\nout[\", j,\"] = \", out\n",
    "            lab = labels[j].detach().numpy()\n",
    "            print \"\\nlab[\", j,\"] = \", lab\n",
    "            diff+= get_mse_diff(out,lab)\n",
    "            print \"\\ndiff = \", diff\n",
    "            print \"\\n\\n------------------------------------------------\\n\\n\"\n",
    "    print \"Loss : \", diff/(batch_size*10) ,\" %\"\n",
    "    print \"Accuracy : \",(1 -diff/(batch_size*10))*100, \" %\"\n",
    "    \n",
    "def get_mse_diff(x,y):\n",
    "    diff= 0\n",
    "    for i in range(6):\n",
    "        diff += (x[i]-y[i])*(x[i]-y[i])\n",
    "    return diff/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVONet(\n",
      "  (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv3_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3_1): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv4_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4_1): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5_1): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (lstm1): LSTMCell(122880, 100)\n",
      "  (lstm2): LSTMCell(100, 100)\n",
      "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Creating model and defining loss and optimizer to be used \n",
    "model = DeepVONet()\n",
    "print(model)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "128\n",
      "128\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "1024\n",
      "1024\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#Uncomment lines below to see model paramters \n",
    "for parameter in model.parameters():\n",
    "    print(len(parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch : ', 1)\n",
      "('Train num :', 1)\n",
      "('Train num :', 2)\n",
      "('Train num :', 3)\n",
      "('Train num :', 4)\n",
      "('Train num :', 5)\n",
      "('Train num :', 6)\n",
      "('Train num :', 7)\n",
      "('Train num :', 8)\n",
      "('Train num :', 9)\n",
      "('Train num :', 10)\n",
      "Epoch : 1 Loss: 0.074\n",
      "('Epoch : ', 2)\n",
      "('Train num :', 1)\n",
      "('Train num :', 2)\n",
      "('Train num :', 3)\n",
      "('Train num :', 4)\n",
      "('Train num :', 5)\n",
      "('Train num :', 6)\n",
      "('Train num :', 7)\n",
      "('Outputs ', tensor([[-0.0614, -0.0207,  0.0390,  0.0232,  0.0298,  0.0556],\n",
      "        [-0.0648, -0.0235,  0.0369,  0.0118,  0.0321,  0.0566],\n",
      "        [-0.0600, -0.0144,  0.0373,  0.0236,  0.0364,  0.0654],\n",
      "        [-0.0561, -0.0283,  0.0413,  0.0247,  0.0290,  0.0629],\n",
      "        [-0.0614, -0.0241,  0.0408,  0.0134,  0.0259,  0.0550],\n",
      "        [-0.0614, -0.0166,  0.0419,  0.0214,  0.0322,  0.0570],\n",
      "        [-0.0594, -0.0257,  0.0384,  0.0179,  0.0323,  0.0547],\n",
      "        [-0.0608, -0.0178,  0.0407,  0.0262,  0.0339,  0.0536],\n",
      "        [-0.0652, -0.0213,  0.0450,  0.0197,  0.0330,  0.0606],\n",
      "        [-0.0582, -0.0241,  0.0388,  0.0246,  0.0349,  0.0586]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 2.0594e-02, -1.4955e-03,  3.9705e-01, -1.7261e-03,  7.7224e-03,\n",
      "          3.9439e-04],\n",
      "        [ 2.0952e-02,  3.5090e-04,  3.7189e-01, -1.3596e-03,  8.0403e-03,\n",
      "         -1.8822e-03],\n",
      "        [ 2.0416e-02,  4.2220e-04,  3.6276e-01, -6.1519e-04,  9.1312e-03,\n",
      "         -3.5769e-03],\n",
      "        [ 1.8255e-02, -1.6629e-03,  3.1108e-01, -3.1642e-04,  8.4387e-03,\n",
      "         -4.7554e-03],\n",
      "        [ 2.3354e-02, -3.9830e-03,  2.9824e-01, -9.7984e-04,  9.0263e-03,\n",
      "         -3.7545e-03],\n",
      "        [ 2.9870e-02, -8.8722e-03,  2.7424e-01, -9.2934e-04,  9.3049e-03,\n",
      "          3.6638e-04],\n",
      "        [ 3.3167e-02, -7.5066e-03,  2.6568e-01,  1.4069e-03,  1.0867e-02,\n",
      "          1.0532e-03],\n",
      "        [ 3.6141e-02, -6.8121e-03,  2.5903e-01,  4.1413e-03,  1.2283e-02,\n",
      "          1.5869e-03],\n",
      "        [ 3.9209e-02, -5.8532e-03,  2.5680e-01,  3.4017e-03,  1.3540e-02,\n",
      "         -6.4545e-04],\n",
      "        [ 4.1005e-02, -7.1255e-03,  2.5469e-01,  7.3729e-04,  1.5821e-02,\n",
      "         -1.7898e-03]]))\n",
      "('Train num :', 8)\n",
      "('Outputs ', tensor([[-0.0667, -0.0162,  0.0489,  0.0196,  0.0277,  0.0529],\n",
      "        [-0.0609, -0.0235,  0.0411,  0.0172,  0.0274,  0.0547],\n",
      "        [-0.0576, -0.0246,  0.0334,  0.0196,  0.0320,  0.0638],\n",
      "        [-0.0556, -0.0171,  0.0385,  0.0247,  0.0311,  0.0633],\n",
      "        [-0.0654, -0.0229,  0.0411,  0.0172,  0.0288,  0.0559],\n",
      "        [-0.0562, -0.0189,  0.0303,  0.0224,  0.0304,  0.0595],\n",
      "        [-0.0595, -0.0216,  0.0377,  0.0273,  0.0303,  0.0590],\n",
      "        [-0.0524, -0.0294,  0.0477,  0.0243,  0.0288,  0.0618],\n",
      "        [-0.0594, -0.0188,  0.0349,  0.0235,  0.0353,  0.0633],\n",
      "        [-0.0616, -0.0232,  0.0389,  0.0171,  0.0408,  0.0539]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 4.8311e-02, -5.5242e-03,  2.6262e-01, -1.3854e-04,  1.7194e-02,\n",
      "         -2.0468e-03],\n",
      "        [ 5.0719e-02, -5.4792e-03,  2.3645e-01,  1.0290e-04,  1.6465e-02,\n",
      "         -4.3896e-04],\n",
      "        [ 6.0554e-02, -5.1905e-03,  2.4443e-01,  1.1376e-04,  1.8530e-02,\n",
      "          7.0562e-04],\n",
      "        [ 6.7503e-02, -5.5289e-03,  2.4675e-01, -7.2085e-04,  2.0364e-02,\n",
      "          7.1496e-04],\n",
      "        [ 6.7030e-02, -4.4935e-03,  2.2994e-01, -1.2318e-04,  2.0068e-02,\n",
      "         -5.8158e-04],\n",
      "        [ 7.3704e-02, -4.9219e-03,  2.3803e-01,  5.3368e-04,  2.1982e-02,\n",
      "         -2.0472e-03],\n",
      "        [ 8.1427e-02, -6.2023e-03,  2.3619e-01,  2.9898e-04,  2.3597e-02,\n",
      "         -1.3084e-03],\n",
      "        [ 9.0274e-02, -7.5956e-03,  2.3871e-01, -1.1262e-03,  2.5586e-02,\n",
      "         -1.6708e-03],\n",
      "        [ 9.5974e-02, -8.5755e-03,  2.3910e-01, -2.0531e-03,  2.7147e-02,\n",
      "         -3.6824e-03],\n",
      "        [ 1.0653e-01, -9.8741e-03,  2.4145e-01, -1.9572e-03,  2.9621e-02,\n",
      "         -3.2723e-03]]))\n",
      "('Train num :', 9)\n",
      "('Outputs ', tensor([[-0.0610, -0.0223,  0.0419,  0.0290,  0.0360,  0.0572],\n",
      "        [-0.0603, -0.0231,  0.0564,  0.0224,  0.0292,  0.0546],\n",
      "        [-0.0576, -0.0254,  0.0465,  0.0157,  0.0262,  0.0606],\n",
      "        [-0.0640, -0.0226,  0.0458,  0.0207,  0.0290,  0.0646],\n",
      "        [-0.0631, -0.0273,  0.0485,  0.0254,  0.0297,  0.0551],\n",
      "        [-0.0615, -0.0252,  0.0507,  0.0212,  0.0257,  0.0579],\n",
      "        [-0.0673, -0.0280,  0.0429,  0.0232,  0.0317,  0.0550],\n",
      "        [-0.0590, -0.0277,  0.0513,  0.0231,  0.0298,  0.0590],\n",
      "        [-0.0610, -0.0238,  0.0502,  0.0249,  0.0356,  0.0610],\n",
      "        [-0.0515, -0.0241,  0.0401,  0.0277,  0.0445,  0.0595]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 0.1150, -0.0105,  0.2291, -0.0014,  0.0303, -0.0014],\n",
      "        [ 0.1307, -0.0098,  0.2261,  0.0005,  0.0326,  0.0018],\n",
      "        [ 0.1451, -0.0091,  0.2245,  0.0018,  0.0362,  0.0031],\n",
      "        [ 0.1519, -0.0090,  0.2199,  0.0026,  0.0379,  0.0012],\n",
      "        [ 0.1628, -0.0095,  0.2187,  0.0023,  0.0397, -0.0009],\n",
      "        [ 0.1793, -0.0107,  0.2251,  0.0010,  0.0418, -0.0018],\n",
      "        [ 0.1951, -0.0122,  0.2287, -0.0011,  0.0439, -0.0039],\n",
      "        [ 0.2071, -0.0155,  0.2220, -0.0040,  0.0449, -0.0067],\n",
      "        [ 0.2294, -0.0165,  0.2320, -0.0054,  0.0466, -0.0082],\n",
      "        [ 0.2566, -0.0180,  0.2266, -0.0030,  0.0492, -0.0036]]))\n",
      "('Train num :', 10)\n",
      "('Outputs ', tensor([[-0.0534, -0.0220,  0.0467,  0.0247,  0.0333,  0.0672],\n",
      "        [-0.0683, -0.0224,  0.0449,  0.0165,  0.0318,  0.0591],\n",
      "        [-0.0588, -0.0217,  0.0503,  0.0206,  0.0258,  0.0651],\n",
      "        [-0.0659, -0.0258,  0.0403,  0.0221,  0.0362,  0.0545],\n",
      "        [-0.0604, -0.0219,  0.0424,  0.0231,  0.0325,  0.0535],\n",
      "        [-0.0601, -0.0259,  0.0410,  0.0166,  0.0328,  0.0541],\n",
      "        [-0.0637, -0.0236,  0.0450,  0.0193,  0.0320,  0.0595],\n",
      "        [-0.0591, -0.0258,  0.0510,  0.0203,  0.0300,  0.0579],\n",
      "        [-0.0626, -0.0266,  0.0502,  0.0196,  0.0299,  0.0592],\n",
      "        [-0.0619, -0.0237,  0.0463,  0.0140,  0.0316,  0.0600]],\n",
      "       grad_fn=<AddmmBackward>))\n",
      "('Labels ', tensor([[ 2.8782e-01, -1.2927e-02,  2.1552e-01, -3.1928e-03,  5.1621e-02,\n",
      "         -5.8455e-04],\n",
      "        [ 3.2365e-01, -1.2920e-04,  2.1624e-01, -2.6275e-03,  5.2530e-02,\n",
      "         -1.3384e-03],\n",
      "        [ 3.4295e-01, -5.4504e-03,  2.0469e-01, -4.2049e-03,  5.5079e-02,\n",
      "         -3.8064e-03],\n",
      "        [ 3.6108e-01,  3.9437e-03,  1.9395e-01, -1.3758e-03,  5.5929e-02,\n",
      "         -1.8820e-03],\n",
      "        [ 3.8550e-01, -6.8134e-03,  1.8574e-01, -5.8764e-03,  5.6795e-02,\n",
      "         -4.8008e-03],\n",
      "        [ 4.0688e-01, -5.3029e-03,  1.6584e-01, -9.7603e-03,  5.5622e-02,\n",
      "         -7.4900e-03],\n",
      "        [ 4.3222e-01, -2.5228e-02,  1.4855e-01, -1.6694e-02,  5.5205e-02,\n",
      "         -1.4350e-02],\n",
      "        [ 4.4306e-01, -2.2998e-02,  1.3449e-01, -2.7119e-02,  5.3026e-02,\n",
      "         -2.6305e-02],\n",
      "        [ 4.6230e-01, -2.4975e-02,  1.0298e-01, -1.4363e-02,  5.2293e-02,\n",
      "         -1.6000e-02],\n",
      "        [ 4.8182e-01, -3.1181e-02,  8.0840e-02,  6.1805e-03,  5.0038e-02,\n",
      "          8.7695e-03]]))\n",
      "Epoch : 2 Loss: 0.067\n",
      "Finished Training\n",
      "Time taken in Training 177.791949987\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_model(model,10,X,y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(), 'DeepVO.pt')\n",
    "\n",
    "#Load model\n",
    "model_loaded = torch.load('DeepVO.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xenial/Datasets/KITTI\n",
      "('images ', '/home/xenial/Datasets/KITTI/sequences/03/image_0')\n",
      "('images count : ', 800)\n",
      "('pose ', '/home/xenial/Datasets/KITTI/poses/03.txt')\n",
      "('poses count: ', 800)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 9437184000 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1511d758c8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVODataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/xenial/Datasets/KITTI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 9437184000 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = VODataLoader(\"/home/xenial/Datasets/KITTI\", test=True)\n",
    "X_test=torch.stack(X_test).view(-1,10,6, 384, 1280)\n",
    "y_test=torch.stack(y_test).view(-1,10,6)\n",
    "print(X_test.size())\n",
    "print(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions from the model \n",
    "test_batch_size = 2 \n",
    "\n",
    "y_output = testing_model(model, test_batch_size, X)\n",
    "\n",
    "print \"y_output size = \" , y_output.size()\n",
    "print \"y_output shape = \" , y_output.shape[0]\n",
    "\n",
    "torch.save(y_output,\"y_output.pt\")\n",
    "print \"output saved successfully!\"\n",
    "\n",
    "#getting accuracy\n",
    "get_accuracy(y_output, y_test, test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
